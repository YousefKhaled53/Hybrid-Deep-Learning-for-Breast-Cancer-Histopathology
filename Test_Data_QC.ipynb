{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1913c0d69738446f91f12c7f78a2f290": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a5fbe3a83e4405cb46ca1a7419b8bbb",
              "IPY_MODEL_24057041be2d4f299e00619b9d2d5989",
              "IPY_MODEL_7db78c6f8e394484ac9d48c640400b1c"
            ],
            "layout": "IPY_MODEL_486c536fe3374ac3abdb238c20c619a7"
          }
        },
        "3a5fbe3a83e4405cb46ca1a7419b8bbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b089dc02c5b409693005687c58a53a5",
            "placeholder": "​",
            "style": "IPY_MODEL_bd03ba2d49b74d2e9efef4ecb7545aff",
            "value": "Processing BreaKHis for Stats: 100%"
          }
        },
        "24057041be2d4f299e00619b9d2d5989": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5846880c12a4450f90d871ae430f5862",
            "max": 2000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5a31e11d85454fae9bd7061791d9cb15",
            "value": 2000
          }
        },
        "7db78c6f8e394484ac9d48c640400b1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f24c4692816b4a188f2c432ae8802141",
            "placeholder": "​",
            "style": "IPY_MODEL_a5074c734b874dbf91ba6d16404085f4",
            "value": " 2000/2000 [25:02&lt;00:00,  1.36it/s]"
          }
        },
        "486c536fe3374ac3abdb238c20c619a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b089dc02c5b409693005687c58a53a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd03ba2d49b74d2e9efef4ecb7545aff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5846880c12a4450f90d871ae430f5862": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a31e11d85454fae9bd7061791d9cb15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f24c4692816b4a188f2c432ae8802141": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5074c734b874dbf91ba6d16404085f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cfa3f557d69d4602ab545bf61560bc6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff820a5ba8ab40a0a48036b9b3916c28",
              "IPY_MODEL_5e7bdc4bca2b4d239c35602609d332f4",
              "IPY_MODEL_32ca5b64b59f4c559523264a78326127"
            ],
            "layout": "IPY_MODEL_f33143b4fb7e4530956ba666b1e00337"
          }
        },
        "ff820a5ba8ab40a0a48036b9b3916c28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75e0102a32ba493cb855a59ae1782a46",
            "placeholder": "​",
            "style": "IPY_MODEL_af11b17cc31248bb84918f2358f015d2",
            "value": "Quality Controlling BACH Patches: 100%"
          }
        },
        "5e7bdc4bca2b4d239c35602609d332f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_588cdd94aecd417b9152dcf6156e0458",
            "max": 440,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4cab4c6baafd461ca27d70f3d19e1da6",
            "value": 440
          }
        },
        "32ca5b64b59f4c559523264a78326127": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a4581cbbfcf47099959d8446acda64c",
            "placeholder": "​",
            "style": "IPY_MODEL_58c18a22d0c44b03a041ac7ecfa93d7e",
            "value": " 440/440 [00:53&lt;00:00,  7.83it/s]"
          }
        },
        "f33143b4fb7e4530956ba666b1e00337": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75e0102a32ba493cb855a59ae1782a46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af11b17cc31248bb84918f2358f015d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "588cdd94aecd417b9152dcf6156e0458": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cab4c6baafd461ca27d70f3d19e1da6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2a4581cbbfcf47099959d8446acda64c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58c18a22d0c44b03a041ac7ecfa93d7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d00679195f9d4d69833bf81c182758c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b9595e32df28441aafb63a468f09e05d",
              "IPY_MODEL_6a17136c754e4b8b877eba25c2305d78",
              "IPY_MODEL_0534529dbab14f0fa960266211d61b50"
            ],
            "layout": "IPY_MODEL_283c8ae248904365b1838f811ada36b7"
          }
        },
        "b9595e32df28441aafb63a468f09e05d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c339c3ed3864e2fa087913c22ec5edf",
            "placeholder": "​",
            "style": "IPY_MODEL_2a1fd8ab0eb448539a08ecb7ffac4dd6",
            "value": "TTA on New Dataset: 100%"
          }
        },
        "6a17136c754e4b8b877eba25c2305d78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ff7291d61414e96a7dd544de21af40d",
            "max": 275,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a468c86b57ca4f2bb86907972f3062a6",
            "value": 275
          }
        },
        "0534529dbab14f0fa960266211d61b50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7c83998645244d9b043276375e6929b",
            "placeholder": "​",
            "style": "IPY_MODEL_e099e0fa0424459cbf750394c098bccc",
            "value": " 275/275 [03:14&lt;00:00,  1.23it/s]"
          }
        },
        "283c8ae248904365b1838f811ada36b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c339c3ed3864e2fa087913c22ec5edf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a1fd8ab0eb448539a08ecb7ffac4dd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ff7291d61414e96a7dd544de21af40d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a468c86b57ca4f2bb86907972f3062a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a7c83998645244d9b043276375e6929b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e099e0fa0424459cbf750394c098bccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6s0nTGhH4uO",
        "outputId": "ffdbe57f-36c1-4341-f79d-2b8ec296ffac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q pandas numpy matplotlib seaborn scikit-learn pillow tqdm opencv-python-headless torch torchvision torchaudio torchinfo\n",
        "!pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djINn2wEIrBC",
        "outputId": "074b31d2-7c54-415b-b852-2228f9e486b8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.4.26)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T_vision\n",
        "import re\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models\n",
        "from torchinfo import summary\n",
        "from PIL import Image\n",
        "import time\n",
        "from google.colab import files\n",
        "import json\n",
        "import gdown\n",
        "print(\"Imports and installations complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wanZBxZ6Ie-Y",
        "outputId": "53269597-70f9-45b1-befe-0632b4919b74"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imports and installations complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GDRIVE_FILE_ID_OR_LINK = \"https://drive.google.com/file/d/1K_mgY-ZtFhv_g0obsqg3chH5TRiLc2ny/view?usp=drive_link\"\n",
        "MODEL_FILENAME_IN_COLAB = \"best_phased_model_checkpoint.pth\" # How it will be named in Colab\n",
        "\n",
        "def download_model_from_gdrive(drive_id_or_link, output_filename, expected_min_size_bytes=100*1024*1024): # Min 100MB example\n",
        "    \"\"\"Downloads a file from Google Drive using gdown and checks size.\"\"\"\n",
        "    if os.path.exists(output_filename):\n",
        "        # Optionally, check size here too if you want to re-download if too small\n",
        "        # current_size = os.path.getsize(output_filename)\n",
        "        # if current_size < expected_min_size_bytes:\n",
        "        #     print(f\"Existing file '{output_filename}' is too small ({current_size} bytes). Re-downloading.\")\n",
        "        #     os.remove(output_filename)\n",
        "        # else:\n",
        "        print(f\"Model '{output_filename}' already exists locally. Skipping download.\")\n",
        "        return True\n",
        "\n",
        "    print(f\"Attempting to download model from Google Drive: {drive_id_or_link} as {output_filename}\")\n",
        "    try:\n",
        "        # Using --fuzzy allows gdown to better handle various Google Drive link formats\n",
        "        # Using output=output_filename is crucial\n",
        "        gdown.download(url=drive_id_or_link, output=output_filename, quiet=False, fuzzy=True)\n",
        "\n",
        "        if os.path.exists(output_filename):\n",
        "            downloaded_size = os.path.getsize(output_filename)\n",
        "            print(f\"Model downloaded successfully as '{output_filename}'. Size: {downloaded_size / (1024*1024):.2f} MB.\")\n",
        "            if downloaded_size < expected_min_size_bytes: # Check if the size is reasonable\n",
        "                print(f\"WARNING: Downloaded file size ({downloaded_size} bytes) is smaller than expected minimum ({expected_min_size_bytes} bytes). File might be incomplete or corrupted.\")\n",
        "                return False # Or raise an error\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"ERROR: Model download failed (file '{output_filename}' not found after gdown attempt).\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR downloading model from Google Drive: {e}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "x5FcKZUfIobl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXPECTED_MIN_MODEL_SIZE_BYTES = 150 * 1024 * 1024 # Example: 150 MB\n",
        "\n",
        "MODEL_DOWNLOADED_SUCCESSFULLY = download_model_from_gdrive(GDRIVE_FILE_ID_OR_LINK, MODEL_FILENAME_IN_COLAB, EXPECTED_MIN_MODEL_SIZE_BYTES)\n",
        "\n",
        "if not MODEL_DOWNLOADED_SUCCESSFULLY:\n",
        "    print(\"---------------------------------------------------------------------------\")\n",
        "    print(\"CRITICAL: Model download failed or the file is too small. Cannot proceed.\")\n",
        "    print(f\"Please check your GDRIVE_FILE_ID_OR_LINK: '{GDRIVE_FILE_ID_OR_LINK}' and sharing permissions.\")\n",
        "    print(\"Then, re-run this cell and ensure the download completes successfully.\")\n",
        "    print(\"---------------------------------------------------------------------------\")\n",
        "    # You might want to raise an error here to stop execution if the model is critical\n",
        "    # raise RuntimeError(\"Model download failed, cannot continue.\")\n",
        "else:\n",
        "    # Now, MODEL_FILENAME_IN_COLAB is the path to use for loading\n",
        "    # MODEL_CHECKPOINT_PATH = MODEL_FILENAME_IN_COLAB # Update this global if you use it elsewhere\n",
        "\n",
        "    # Example: (This would be inside your run_bach_evaluation_v0 or similar)\n",
        "    # model_to_evaluate, le_classes_cp = load_trained_model_from_path_v0(\n",
        "    #     MODEL_FILENAME_IN_COLAB, # Use the local filename\n",
        "    #     num_classes_expected=NUM_CLASSES_MODEL\n",
        "    # )\n",
        "    pass # Continue with the rest of your script logic that loads the model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YK9mN3tCIyvh",
        "outputId": "42b5ff42-8134-4509-83e0-ae0af3078971"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to download model from Google Drive: https://drive.google.com/file/d/1K_mgY-ZtFhv_g0obsqg3chH5TRiLc2ny/view?usp=drive_link as best_phased_model_checkpoint.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1K_mgY-ZtFhv_g0obsqg3chH5TRiLc2ny\n",
            "From (redirected): https://drive.google.com/uc?id=1K_mgY-ZtFhv_g0obsqg3chH5TRiLc2ny&confirm=t&uuid=3929b676-5b6e-4417-bb52-a58676e8a0c8\n",
            "To: /content/best_phased_model_checkpoint.pth\n",
            "100%|██████████| 850M/850M [00:08<00:00, 104MB/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model downloaded successfully as 'best_phased_model_checkpoint.pth'. Size: 810.99 MB.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "MODEL_CHECKPOINT_PATH = \"best_phased_model_checkpoint.pth\" # Expect this to be uploaded by you\n",
        "\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "BATCH_SIZE_FOR_STATS_CALC = 64\n",
        "NUM_WORKERS_PROCESSING = 2\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Label mapping (must match your original BreaKHis LabelEncoder)\n",
        "ORIGINAL_BREAKHIS_LABEL_CLASSES = ['benign', 'malignant'] # e.g. benign=0, malignant=1\n",
        "NUM_CLASSES_MODEL = len(ORIGINAL_BREAKHIS_LABEL_CLASSES)\n",
        "print(f\"Model trained for classes: {ORIGINAL_BREAKHIS_LABEL_CLASSES}\")\n",
        "\n",
        "# Placeholder for BreaKHis global stats - will be calculated\n",
        "breakhis_global_lab_stats = None\n",
        "\n",
        "print(\"Configuration set.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xPrZm4dIg-Q",
        "outputId": "9be97757-ded7-4778-a7f2-a5d62921538f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Model trained for classes: ['benign', 'malignant']\n",
            "Configuration set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_kaggle_api():\n",
        "    print(\"Setting up Kaggle API...\")\n",
        "    if os.path.exists('/root/.kaggle/kaggle.json'):\n",
        "        print(\"Kaggle API already configured.\")\n",
        "        return True\n",
        "    try:\n",
        "        os.remove('/root/.kaggle/kaggle.json')\n",
        "    except OSError: pass\n",
        "    try:\n",
        "        print(\"Please upload your kaggle.json API key:\")\n",
        "        uploaded = files.upload()\n",
        "        if 'kaggle.json' not in uploaded:\n",
        "            raise FileNotFoundError(\"kaggle.json not found.\")\n",
        "        !mkdir -p ~/.kaggle\n",
        "        !cp kaggle.json ~/.kaggle/\n",
        "        !chmod 600 ~/.kaggle/kaggle.json\n",
        "        print(\"Kaggle API configured successfully.\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Kaggle setup failed: {e}.\")\n",
        "        return False\n",
        "\n",
        "KAGGLE_READY = setup_kaggle_api()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "2fA6fMryI4J2",
        "outputId": "90d22ff1-0836-46b8-e9d7-065be3fbfde9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up Kaggle API...\n",
            "Please upload your kaggle.json API key:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-875dcd63-673f-404a-b2ea-ad00b5562bbd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-875dcd63-673f-404a-b2ea-ad00b5562bbd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Kaggle API configured successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_organize_breakhis_for_stats():\n",
        "    if not KAGGLE_READY:\n",
        "        print(\"Skipping BreaKHis download as Kaggle API is not ready.\")\n",
        "        return None\n",
        "\n",
        "    organized_images_root_dir = '/content/organized_breakhis_for_stats'\n",
        "    benign_check_path = os.path.join(organized_images_root_dir, 'benign')\n",
        "    if os.path.exists(benign_check_path) and os.listdir(benign_check_path):\n",
        "        print(f\"BreaKHis images already found in '{organized_images_root_dir}'. Skipping download/organization.\")\n",
        "        return organized_images_root_dir\n",
        "\n",
        "    print(\"Downloading BreaKHis dataset (for stats)...\")\n",
        "    if os.path.exists(\"breakhis.zip\"): os.remove(\"breakhis.zip\")\n",
        "    if os.path.exists(\"/content/breakhis_unzipped_temp\"): shutil.rmtree(\"/content/breakhis_unzipped_temp\")\n",
        "    if os.path.exists(organized_images_root_dir): shutil.rmtree(organized_images_root_dir)\n",
        "\n",
        "    !kaggle datasets download -d ambarish/breakhis -p /content/ -q # Downloads breakhis.zip to /content/\n",
        "    if not os.path.exists(\"/content/breakhis.zip\"):\n",
        "        print(\"ERROR: breakhis.zip not found after download.\")\n",
        "        return None\n",
        "\n",
        "    print(\"Unzipping BreaKHis dataset...\")\n",
        "    os.makedirs(\"/content/breakhis_unzipped_temp\", exist_ok=True)\n",
        "    !unzip -q /content/breakhis.zip -d /content/breakhis_unzipped_temp\n",
        "    # os.remove(\"/content/breakhis.zip\") # Clean up zip\n",
        "\n",
        "    source_base_dir_bh = '/content/breakhis_unzipped_temp/BreaKHis_v1/BreaKHis_v1/histology_slides/breast'\n",
        "    if not os.path.exists(source_base_dir_bh):\n",
        "        print(f\"ERROR: BreaKHis source path not found: {source_base_dir_bh}\")\n",
        "        print(\"Listing contents of /content/breakhis_unzipped_temp to debug:\")\n",
        "        !ls -R /content/breakhis_unzipped_temp\n",
        "        return None\n",
        "\n",
        "    os.makedirs(os.path.join(organized_images_root_dir, 'benign'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(organized_images_root_dir, 'malignant'), exist_ok=True)\n",
        "\n",
        "    def copy_images_local(source_path, destination_path_class):\n",
        "        print(f\"Copying from {source_path} to {destination_path_class}\")\n",
        "        for root, _, files_list in os.walk(source_path):\n",
        "            for file_item in files_list:\n",
        "                if file_item.lower().endswith('.png'):\n",
        "                    src_file = os.path.join(root, file_item)\n",
        "                    dst_file = os.path.join(destination_path_class, file_item)\n",
        "                    counter = 1\n",
        "                    base_name, ext = os.path.splitext(file_item)\n",
        "                    while os.path.exists(dst_file):\n",
        "                        dst_file = os.path.join(destination_path_class, f\"{base_name}_{counter}{ext}\")\n",
        "                        counter += 1\n",
        "                    shutil.copy2(src_file, dst_file)\n",
        "        print(f\"Finished copying to {destination_path_class}\")\n",
        "\n",
        "    copy_images_local(os.path.join(source_base_dir_bh, 'benign'), os.path.join(organized_images_root_dir, 'benign'))\n",
        "    copy_images_local(os.path.join(source_base_dir_bh, 'malignant'), os.path.join(organized_images_root_dir, 'malignant'))\n",
        "    print(\"BreaKHis images organized for stats calculation.\")\n",
        "    shutil.rmtree(\"/content/breakhis_unzipped_temp\") # Clean up temp unzip dir\n",
        "    return organized_images_root_dir"
      ],
      "metadata": {
        "id": "-EWPUgSwI7vN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_breakhis_stats_metadata(organized_dir):\n",
        "    data = []\n",
        "    for label_type in ['benign', 'malignant']:\n",
        "        class_dir = os.path.join(organized_dir, label_type)\n",
        "        if not os.path.isdir(class_dir): continue\n",
        "        for fname in os.listdir(class_dir):\n",
        "            if fname.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".tif\")): # Accept common image formats\n",
        "                data.append({'image_path': os.path.join(class_dir, fname)})\n",
        "    if not data: return None\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# --- Calculate BreaKHis LAB Statistics (Online version) ---\n",
        "def calculate_dataset_lab_stats_online(metadata_df, sample_size=None):\n",
        "    print(f\"Calculating LAB statistics ONLINE from {len(metadata_df)} images (sample_size={sample_size or len(metadata_df)})...\")\n",
        "    channel_stats = [{'count': 0, 'mean': 0.0, 'M2': 0.0} for _ in range(3)]\n",
        "    df_to_sample = metadata_df\n",
        "    if sample_size and len(metadata_df) > sample_size:\n",
        "        df_to_sample = metadata_df.sample(n=sample_size, random_state=SEED)\n",
        "\n",
        "    for img_path in tqdm(df_to_sample['image_path'], desc=\"Processing BreaKHis for Stats\"):\n",
        "        try:\n",
        "            img_pil = Image.open(img_path).convert('RGB')\n",
        "            img_np_rgb = np.array(img_pil)\n",
        "            img_lab = cv2.cvtColor(img_np_rgb, cv2.COLOR_RGB2LAB).astype(float)\n",
        "            for i in range(3):\n",
        "                pixels = img_lab[:,:,i].flatten()\n",
        "                for x in pixels:\n",
        "                    channel_stats[i]['count'] += 1\n",
        "                    delta = x - channel_stats[i]['mean']\n",
        "                    channel_stats[i]['mean'] += delta / channel_stats[i]['count']\n",
        "                    delta2 = x - channel_stats[i]['mean']\n",
        "                    channel_stats[i]['M2'] += delta * delta2\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {img_path} for stats due to error: {e}\")\n",
        "\n",
        "    final_means, final_stds = [], []\n",
        "    all_counts_zero = all(cs['count'] == 0 for cs in channel_stats)\n",
        "    if all_counts_zero:\n",
        "        print(\"Error: No pixel data processed from BreaKHis. Cannot compute statistics.\")\n",
        "        return None\n",
        "\n",
        "    for i in range(3):\n",
        "        count, mean_val, M2_val = channel_stats[i]['count'], channel_stats[i]['mean'], channel_stats[i]['M2']\n",
        "        final_means.append(mean_val if count > 0 else 0.0)\n",
        "        variance = M2_val / count if count > 0 else 0\n",
        "        final_stds.append(np.sqrt(variance) if variance >= 0 else 0.0)\n",
        "    stats = {'means': final_means, 'stds': final_stds}\n",
        "    print(f\"Calculated BreaKHis Dataset LAB Means (Online): {stats['means']}\")\n",
        "    print(f\"Calculated BreaKHis Dataset LAB Stds (Online): {stats['stds']}\")\n",
        "    return stats"
      ],
      "metadata": {
        "id": "dYzI4H8YJBpw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if KAGGLE_READY:\n",
        "    organized_breakhis_path = download_and_organize_breakhis_for_stats()\n",
        "    if organized_breakhis_path:\n",
        "        breakhis_stats_df = create_breakhis_stats_metadata(organized_breakhis_path)\n",
        "        if breakhis_stats_df is not None and not breakhis_stats_df.empty:\n",
        "            breakhis_global_lab_stats = calculate_dataset_lab_stats_online(breakhis_stats_df, sample_size=2000) # Sample BreaKHis\n",
        "if breakhis_global_lab_stats is None:\n",
        "    print(\"Using FALLBACK SIMULATED BreaKHis LAB stats as actual calculation failed or was skipped.\")\n",
        "    breakhis_global_lab_stats = {'means': [150.0, 128.0, 128.0], 'stds': [40.0, 15.0, 15.0]}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257,
          "referenced_widgets": [
            "1913c0d69738446f91f12c7f78a2f290",
            "3a5fbe3a83e4405cb46ca1a7419b8bbb",
            "24057041be2d4f299e00619b9d2d5989",
            "7db78c6f8e394484ac9d48c640400b1c",
            "486c536fe3374ac3abdb238c20c619a7",
            "3b089dc02c5b409693005687c58a53a5",
            "bd03ba2d49b74d2e9efef4ecb7545aff",
            "5846880c12a4450f90d871ae430f5862",
            "5a31e11d85454fae9bd7061791d9cb15",
            "f24c4692816b4a188f2c432ae8802141",
            "a5074c734b874dbf91ba6d16404085f4"
          ]
        },
        "id": "inCE0hnWJFP6",
        "outputId": "6783b33b-727b-47f4-85df-bcb9cfbf7f40"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading BreaKHis dataset (for stats)...\n",
            "Dataset URL: https://www.kaggle.com/datasets/ambarish/breakhis\n",
            "License(s): unknown\n",
            "Unzipping BreaKHis dataset...\n",
            "Copying from /content/breakhis_unzipped_temp/BreaKHis_v1/BreaKHis_v1/histology_slides/breast/benign to /content/organized_breakhis_for_stats/benign\n",
            "Finished copying to /content/organized_breakhis_for_stats/benign\n",
            "Copying from /content/breakhis_unzipped_temp/BreaKHis_v1/BreaKHis_v1/histology_slides/breast/malignant to /content/organized_breakhis_for_stats/malignant\n",
            "Finished copying to /content/organized_breakhis_for_stats/malignant\n",
            "BreaKHis images organized for stats calculation.\n",
            "Calculating LAB statistics ONLINE from 7909 images (sample_size=2000)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing BreaKHis for Stats:   0%|          | 0/2000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1913c0d69738446f91f12c7f78a2f290"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated BreaKHis Dataset LAB Means (Online): [np.float64(180.77303722737406), np.float64(148.5958863834724), np.float64(116.52933380071916)]\n",
            "Calculated BreaKHis Dataset LAB Stds (Online): [np.float64(35.43960364837374), np.float64(17.545317519936784), np.float64(11.058753513174233)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HybridModel(nn.Module):\n",
        "    def __init__(self, backbone1, backbone2, num_classes):\n",
        "        super().__init__()\n",
        "        self.effnet_feature_extractor = backbone1.features\n",
        "        self.backbone2 = backbone2\n",
        "        vit_original_head = self.backbone2.heads\n",
        "        self.backbone2.heads = nn.Identity()\n",
        "\n",
        "        if isinstance(backbone1.classifier, nn.Sequential) and \\\n",
        "           len(backbone1.classifier) > 1 and \\\n",
        "           isinstance(backbone1.classifier[1], nn.Linear):\n",
        "            effnet_out_dim = backbone1.classifier[1].in_features\n",
        "        elif isinstance(backbone1.classifier, nn.Linear):\n",
        "             effnet_out_dim = backbone1.classifier.in_features\n",
        "        else:\n",
        "            raise AttributeError(\"Could not determine EfficientNet output features.\")\n",
        "\n",
        "        if hasattr(vit_original_head, 'head') and isinstance(vit_original_head.head, nn.Linear):\n",
        "            vit_out_dim = vit_original_head.head.in_features\n",
        "        else:\n",
        "            vit_out_dim = 768\n",
        "\n",
        "        fusion_input_dim = effnet_out_dim + vit_out_dim\n",
        "        fusion_hidden_dim = 1024\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.LayerNorm(fusion_input_dim),\n",
        "            nn.Linear(fusion_input_dim, fusion_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(fusion_hidden_dim),\n",
        "            nn.Dropout(0.6),\n",
        "            nn.Linear(fusion_hidden_dim, fusion_hidden_dim)\n",
        "        )\n",
        "        classifier_hidden_dim = 512\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(fusion_hidden_dim),\n",
        "            nn.Linear(fusion_hidden_dim, classifier_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.7),\n",
        "            nn.Linear(classifier_hidden_dim, num_classes)\n",
        "        )\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "    def forward(self, x_image):\n",
        "        effnet_features = self.effnet_feature_extractor(x_image)\n",
        "        effnet_pooled = self.adaptive_pool(effnet_features)\n",
        "        effnet_flat = torch.flatten(effnet_pooled, 1)\n",
        "\n",
        "        if hasattr(self.backbone2, 'forward_features'):\n",
        "            vit_output = self.backbone2.forward_features(x_image)\n",
        "        else:\n",
        "            x2_processed = self.backbone2._process_input(x_image)\n",
        "            n = x2_processed.shape[0]\n",
        "            batch_class_token = self.backbone2.class_token.expand(n, -1, -1)\n",
        "            x2_tokens = torch.cat([batch_class_token, x2_processed], dim=1)\n",
        "            x2_encoded = self.backbone2.encoder(x2_tokens)\n",
        "            vit_output = x2_encoded\n",
        "\n",
        "        if vit_output.ndim == 3:\n",
        "            vit_cls_token = vit_output[:, 0]\n",
        "        else:\n",
        "            vit_cls_token = vit_output\n",
        "        combined_features = torch.cat([effnet_flat, vit_cls_token], dim=1)\n",
        "        fused_features = self.fusion(combined_features)\n",
        "        output_logits = self.classifier(fused_features)\n",
        "        return output_logits\n",
        "print(\"Original HybridModel class defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYAvy6uHPb2U",
        "outputId": "6d9456a0-07d7-4a24-a066-5434baf5449b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original HybridModel class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def opencv_reinhard_normalize(source_img_np_rgb, target_lab_stats):\n",
        "    if source_img_np_rgb.dtype != np.uint8:\n",
        "        source_img_np_rgb = source_img_np_rgb.astype(np.uint8)\n",
        "    source_lab = cv2.cvtColor(source_img_np_rgb, cv2.COLOR_RGB2LAB).astype(float)\n",
        "    target_means = target_lab_stats['means']\n",
        "    target_stds = target_lab_stats['stds']\n",
        "    normalized_lab = np.zeros_like(source_lab)\n",
        "    for i in range(3):\n",
        "        source_mean = np.mean(source_lab[:,:,i])\n",
        "        source_std  = np.std(source_lab[:,:,i])\n",
        "        channel = source_lab[:,:,i]\n",
        "        channel = (channel - source_mean) * (target_stds[i] / (source_std + 1e-7)) + target_means[i]\n",
        "        normalized_lab[:,:,i] = channel\n",
        "    normalized_lab[:,:,0] = np.clip(normalized_lab[:,:,0], 0, 255)\n",
        "    normalized_lab[:,:,1] = np.clip(normalized_lab[:,:,1], 0, 255)\n",
        "    normalized_lab[:,:,2] = np.clip(normalized_lab[:,:,2], 0, 255)\n",
        "    normalized_rgb = cv2.cvtColor(normalized_lab.astype(np.uint8), cv2.COLOR_LAB2RGB)\n",
        "    return normalized_rgb"
      ],
      "metadata": {
        "id": "Zu9zrKCPP5Du"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NewDatasetForTTA(Dataset):\n",
        "    def __init__(self, metadata_df, use_stain_normalization=False, target_lab_stats_for_norm=None):\n",
        "        self.metadata = metadata_df.reset_index(drop=True)\n",
        "        self.use_stain_normalization = use_stain_normalization\n",
        "        self.target_lab_stats_for_norm = target_lab_stats_for_norm\n",
        "        if self.use_stain_normalization and self.target_lab_stats_for_norm is None:\n",
        "            print(\"Warning: Stain norm enabled for NewDatasetForTTA but no target_lab_stats provided. Will be skipped.\")\n",
        "            self.use_stain_normalization = False\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.metadata.iloc[idx]['image_path']\n",
        "        label = self.metadata.iloc[idx]['encoded_label']\n",
        "        try:\n",
        "            image_pil = Image.open(img_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR loading image {img_path} for TTA: {e}\")\n",
        "            return None, None\n",
        "        if self.use_stain_normalization and self.target_lab_stats_for_norm:\n",
        "            try:\n",
        "                image_np_rgb = np.array(image_pil)\n",
        "                if not (image_np_rgb.shape[0] < 16 or image_np_rgb.shape[1] < 16) :\n",
        "                    normalized_rgb_np = opencv_reinhard_normalize(image_np_rgb, self.target_lab_stats_for_norm)\n",
        "                    image_pil = Image.fromarray(normalized_rgb_np)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during TTA stain normalization for {img_path}: {e}. Using original PIL.\")\n",
        "        return image_pil, label\n",
        "print(\"Normalization helper and NewDatasetForTTA defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fD1pX938P913",
        "outputId": "cbf49f7d-0b8f-47d1-e3e3-5c401cf08052"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalization helper and NewDatasetForTTA defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_trained_model_from_path_v0(checkpoint_path_local, num_classes_expected):\n",
        "    if not os.path.exists(checkpoint_path_local):\n",
        "        print(f\"ERROR: Model checkpoint file not found locally at {checkpoint_path_local}.\")\n",
        "        return None, None\n",
        "\n",
        "    effnet_base = models.efficientnet_v2_s(weights=None)\n",
        "    vit_base = models.vit_b_16(weights=None)\n",
        "\n",
        "    # Instantiate model (it will be on CPU by default)\n",
        "    model = HybridModel(\n",
        "        backbone1=effnet_base,\n",
        "        backbone2=vit_base,\n",
        "        num_classes=num_classes_expected\n",
        "    )\n",
        "    print(f\"Loading model checkpoint from: {checkpoint_path_local}\")\n",
        "    try:\n",
        "        # Load checkpoint TO THE CPU FIRST, then load state_dict, then move model to DEVICE\n",
        "        # This is often more robust than map_location=DEVICE if the model itself isn't on DEVICE yet.\n",
        "        # Alternatively, ensure model is on DEVICE before load_state_dict if map_location=DEVICE is used.\n",
        "        checkpoint = torch.load(checkpoint_path_local, map_location=torch.device('cpu'), weights_only=False) # Load to CPU\n",
        "\n",
        "        if 'model_state_dict' in checkpoint:\n",
        "            model.load_state_dict(checkpoint['model_state_dict']) # Load into CPU model\n",
        "            print(\"Model state_dict loaded successfully (to CPU model instance).\")\n",
        "\n",
        "            # === NOW MOVE THE ENTIRE MODEL TO THE TARGET DEVICE ===\n",
        "            model.to(DEVICE)\n",
        "            print(f\"Model moved to {DEVICE}.\")\n",
        "            # ======================================================\n",
        "\n",
        "            le_classes_from_cp = checkpoint.get('label_encoder_classes', None)\n",
        "            if le_classes_from_cp is not None and isinstance(le_classes_from_cp, np.ndarray):\n",
        "                le_classes_from_cp = le_classes_from_cp.tolist()\n",
        "            return model, le_classes_from_cp\n",
        "        else:\n",
        "            print(\"ERROR: 'model_state_dict' not found in checkpoint.\")\n",
        "            return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR loading checkpoint: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc() # Add this for more detail\n",
        "        return None, None\n",
        "\n",
        "def prepare_new_dataset_metadata(new_dataset_base_dir, breakhis_le_classes_list):\n",
        "    print(f\"Preparing metadata for new dataset from base: {new_dataset_base_dir}\")\n",
        "    data = []\n",
        "    # This new dataset has 'Benign' and 'Malignant' subfolders\n",
        "    for class_name_str in breakhis_le_classes_list: # e.g., 'benign', 'malignant'\n",
        "        # The Kaggle dataset uses title case for folder names\n",
        "        class_folder_name = class_name_str.capitalize()\n",
        "        class_dir_path = os.path.join(new_dataset_base_dir, class_folder_name)\n",
        "\n",
        "        if not os.path.isdir(class_dir_path):\n",
        "            print(f\"Warning: Expected class directory not found: {class_dir_path}\")\n",
        "            continue\n",
        "\n",
        "        for filename in os.listdir(class_dir_path):\n",
        "            if filename.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".tif\")): # Common image extensions\n",
        "                img_path = os.path.join(class_dir_path, filename)\n",
        "                data.append({'image_path': img_path, 'label_str': class_name_str})\n",
        "\n",
        "    if not data:\n",
        "        print(f\"ERROR: No images found in {new_dataset_base_dir} with expected subfolder structure.\")\n",
        "        return None\n",
        "\n",
        "    new_df = pd.DataFrame(data)\n",
        "\n",
        "    # Encode labels using BreaKHis LabelEncoder logic\n",
        "    temp_le = LabelEncoder()\n",
        "    temp_le.classes_ = np.array(breakhis_le_classes_list)\n",
        "    try:\n",
        "        new_df['encoded_label'] = temp_le.transform(new_df['label_str'])\n",
        "    except ValueError as e:\n",
        "        print(f\"ERROR encoding new dataset labels: {e}.\")\n",
        "        print(f\"BreaKHis LE classes: {temp_le.classes_}\")\n",
        "        print(f\"Unique string labels in new dataset: {new_df['label_str'].unique()}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Prepared new dataset metadata with {len(new_df)} images.\")\n",
        "    return new_df[['image_path', 'encoded_label']]\n",
        "\n",
        "print(\"Model loading and new dataset metadata functions defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8P6eWNTyQQfH",
        "outputId": "1625e525-e4bf-48e9-8a5e-abd52655d2c4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loading and new dataset metadata functions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tta_pil_transforms = [\n",
        "    T_vision.Compose([]),\n",
        "    T_vision.RandomHorizontalFlip(p=1.0),\n",
        "    T_vision.RandomVerticalFlip(p=1.0),\n",
        "    T_vision.Compose([T_vision.RandomHorizontalFlip(p=1.0), T_vision.RandomVerticalFlip(p=1.0)]),\n",
        "    T_vision.RandomRotation(degrees=(90,90)),\n",
        "    T_vision.RandomRotation(degrees=(180,180)),\n",
        "    T_vision.RandomRotation(degrees=(270,270)),\n",
        "]\n",
        "print(f\"Defined {len(tta_pil_transforms)} TTA PIL variants.\")\n",
        "\n",
        "final_tta_processing_transform = T_vision.Compose([\n",
        "    T_vision.Resize((224, 224)),\n",
        "    T_vision.ToTensor(),\n",
        "    T_vision.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
        "])\n",
        "print(\"Defined final TTA processing transform.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QChkocHDQT_J",
        "outputId": "5183ba92-90a7-4ef3-c7b3-9a9f8c5d2f8f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined 7 TTA PIL variants.\n",
            "Defined final TTA processing transform.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_new_dataset_metadata(new_dataset_root_unzip_dir, breakhis_le_classes_list):\n",
        "    \"\"\"\n",
        "    Prepares metadata for the \"Breast Cancer Cell (Histopathological images)\" dataset from Kaggle.\n",
        "    Assumes images are in a subfolder like 'Breast Cancer Histopathological Images/Images/'\n",
        "    and filenames contain 'benign' or 'malignant'.\n",
        "    \"\"\"\n",
        "    print(f\"Preparing metadata for new dataset from root unzip: {new_dataset_root_unzip_dir}\")\n",
        "    data = []\n",
        "\n",
        "    # Path to the directory containing all image files\n",
        "    # Based on your example: /content/new_cancer_cell_dataset/Breast Cancer Histopathological Images/Images/\n",
        "    actual_image_dir = os.path.join(new_dataset_root_unzip_dir, \"Breast Cancer Histopathological Images\", \"Images\")\n",
        "\n",
        "    if not os.path.isdir(actual_image_dir):\n",
        "        print(f\"ERROR: Actual image directory not found: {actual_image_dir}\")\n",
        "        print(\"Please verify the unzipped structure. Listing contents of unzip root:\")\n",
        "        if os.path.exists(new_dataset_root_unzip_dir):\n",
        "            !ls -R \"{new_dataset_root_unzip_dir}\" # List to help debug path\n",
        "        return None\n",
        "\n",
        "    print(f\"Scanning for images in: {actual_image_dir}\")\n",
        "    found_images_count = 0\n",
        "\n",
        "    for filename in os.listdir(actual_image_dir):\n",
        "        # Check for common image extensions, including .tif and .tiff\n",
        "        if filename.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\")):\n",
        "            img_path = os.path.join(actual_image_dir, filename)\n",
        "            label_str = None\n",
        "\n",
        "            # Parse label from filename (case-insensitive check)\n",
        "            fn_lower = filename.lower()\n",
        "            # Check for 'benign' ensuring it's not part of a longer word by mistake if patterns are complex\n",
        "            # For this dataset, simple \"in\" check is likely fine\n",
        "            if \"benign\" in fn_lower:\n",
        "                # Map to the string 'benign' as defined in breakhis_le_classes_list[0]\n",
        "                label_str = breakhis_le_classes_list[0]\n",
        "            elif \"malignant\" in fn_lower: # Check if \"malignant\" is in the filename\n",
        "                # Map to the string 'malignant' as defined in breakhis_le_classes_list[1]\n",
        "                label_str = breakhis_le_classes_list[1]\n",
        "\n",
        "            if label_str:\n",
        "                data.append({'image_path': img_path, 'label_str': label_str})\n",
        "                found_images_count += 1\n",
        "            # else: # Optional: Print files for which labels couldn't be parsed\n",
        "            #     print(f\"Warning: Could not determine label for {filename}. Skipping.\")\n",
        "\n",
        "    if not data:\n",
        "        print(f\"ERROR: No images with parsable labels ('benign' or 'malignant' in filename) found in {actual_image_dir}.\")\n",
        "        return None\n",
        "\n",
        "    new_df = pd.DataFrame(data)\n",
        "    print(f\"Found {found_images_count} images with parsable labels from filenames.\")\n",
        "\n",
        "    # Encode string labels ('benign', 'malignant') to numeric (0, 1)\n",
        "    # using the BreaKHis LabelEncoder's class ordering\n",
        "    temp_le = LabelEncoder()\n",
        "    temp_le.classes_ = np.array(breakhis_le_classes_list) # e.g., ['benign', 'malignant']\n",
        "    try:\n",
        "        new_df['encoded_label'] = temp_le.transform(new_df['label_str'])\n",
        "    except ValueError as e:\n",
        "        print(f\"ERROR encoding new dataset labels: {e}.\")\n",
        "        print(f\"Ensure breakhis_le_classes_list ({breakhis_le_classes_list}) matches the parsed label_str values.\")\n",
        "        print(f\"Unique string labels parsed from filenames: {new_df['label_str'].unique()}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Prepared new dataset metadata with {len(new_df)} images.\")\n",
        "    # For this model version, we only need image_path and encoded_label\n",
        "    return new_df[['image_path', 'encoded_label']]\n",
        "\n",
        "    # This dataset directly unzips into Benign/Malignant folders at the root of unzip dir\n",
        "    new_dataset_metadata_df = prepare_bach_metadata_v0(\n",
        "        NEW_DATASET_UNZIP_DIR, # Pass the root where \"main_folder\" is expected\n",
        "        current_le_classes     # e.g., ['benign', 'malignant']\n",
        "    )\n",
        "\n",
        "    if new_dataset_metadata_df is None or new_dataset_metadata_df.empty:\n",
        "        print(\"Failed to prepare metadata for the new dataset. Skipping evaluation.\")\n",
        "        return\n",
        "    # Create Dataset for TTA\n",
        "    new_eval_dataset_tta = NewDatasetForTTA(\n",
        "        metadata_df=new_dataset_metadata_df,\n",
        "        use_stain_normalization=True, # Enable stain normalization\n",
        "        target_lab_stats_for_norm=breakhis_global_lab_stats # Use BreaKHis stats\n",
        "    )\n",
        "    print(f\"Created new dataset for TTA with {len(new_eval_dataset_tta)} images.\")\n",
        "\n",
        "    # Evaluate on New Dataset with TTA\n",
        "    print(\"\\nStarting evaluation on the new dataset with TTA...\")\n",
        "    all_preds_new_tta, all_labels_new_tta, all_tta_avg_probs_new = [], [], []\n",
        "    model_to_evaluate.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(len(new_eval_dataset_tta)), desc=\"TTA on New Dataset\"):\n",
        "            data_tuple = new_eval_dataset_tta[i]\n",
        "            original_pil_image, label = data_tuple[0], data_tuple[1]\n",
        "            if original_pil_image is None: continue\n",
        "\n",
        "            tta_image_tensors = []\n",
        "            for tta_transform_pil in tta_pil_transforms:\n",
        "                augmented_pil = tta_transform_pil(original_pil_image)\n",
        "                image_tensor = final_tta_processing_transform(augmented_pil)\n",
        "                tta_image_tensors.append(image_tensor)\n",
        "            if not tta_image_tensors: continue\n",
        "\n",
        "            tta_batch = torch.stack(tta_image_tensors).to(DEVICE)\n",
        "            outputs = model_to_evaluate(tta_batch)\n",
        "            probabilities = torch.softmax(outputs, dim=1)\n",
        "            avg_probabilities = torch.mean(probabilities, dim=0)\n",
        "            _, predicted_class_idx = torch.max(avg_probabilities, dim=0)\n",
        "\n",
        "            all_preds_new_tta.append(predicted_class_idx.cpu().item())\n",
        "            all_labels_new_tta.append(label)\n",
        "            all_tta_avg_probs_new.append(avg_probabilities.cpu().numpy()[1])\n",
        "\n",
        "    if all_labels_new_tta:\n",
        "        print(\"\\n--- New Dataset Evaluation Results (TTA, BreaKHis Stats Norm) ---\")\n",
        "        new_accuracy = accuracy_score(all_labels_new_tta, all_preds_new_tta)\n",
        "        new_auc = 0.0\n",
        "        if len(np.unique(all_labels_new_tta)) > 1:\n",
        "             new_auc = roc_auc_score(all_labels_new_tta, all_tta_avg_probs_new)\n",
        "        else: print(\"Warning: Could not compute AUC for new dataset (single class).\")\n",
        "        print(f\"  Accuracy:  {new_accuracy * 100:.2f}%\")\n",
        "        print(f\"  AUC:       {new_auc:.4f}\")\n",
        "        # ... (add P, R, F1 as before) ...\n",
        "        print(\"\\nClassification Report (New Dataset with TTA):\")\n",
        "        print(classification_report(all_labels_new_tta, all_preds_new_tta, target_names=current_le_classes, zero_division=0))\n",
        "        cm_new = confusion_matrix(all_labels_new_tta, all_preds_new_tta)\n",
        "        plt.figure(figsize=(6,5)); sns.heatmap(cm_new, annot=True, fmt='d', cmap='Blues', xticklabels=current_le_classes, yticklabels=current_le_classes)\n",
        "        plt.xlabel('Predicted'); plt.ylabel('True'); plt.title('Confusion Matrix - New Dataset (TTA)'); plt.show()\n",
        "    else:\n",
        "        print(\"No valid predictions for new dataset TTA evaluation.\")\n"
      ],
      "metadata": {
        "id": "gFKMoqr-QVnA"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In Cell 6\n",
        "\n",
        "def prepare_bach_metadata_v0(new_dataset_unzip_root, le_breakhis_classes_list,\n",
        "                             subset_size=None, # For overall subsetting AFTER QC\n",
        "                             blur_threshold=75.0, # Lower values are more blurry\n",
        "                             tissue_percentage_threshold=40.0): # Min % of non-background pixels\n",
        "    \"\"\"\n",
        "    Prepares metadata for the 'dina0808/bach-icar-2018' Kaggle dataset.\n",
        "    Includes Quality Control: Blur detection and Tissue area detection.\n",
        "    Assumes structure: new_dataset_unzip_root / main_folder / [train_folder|testing_folder] / [Benign|Normal|InSitu|Invasive] / image.tif\n",
        "    \"\"\"\n",
        "    print(f\"Preparing BACH (dina0808) metadata from unzipped root: {new_dataset_unzip_root}\")\n",
        "    print(f\"QC Params: Blur Threshold={blur_threshold}, Tissue Area Threshold={tissue_percentage_threshold}%\")\n",
        "\n",
        "    all_image_data = [] # To collect all found images before QC\n",
        "\n",
        "    main_folder_path = os.path.join(new_dataset_unzip_root, \"main_folder\")\n",
        "    if not os.path.isdir(main_folder_path):\n",
        "        print(f\"ERROR: 'main_folder' not found in {new_dataset_unzip_root}\")\n",
        "        if os.path.exists(new_dataset_unzip_root):\n",
        "          !ls -R \"{new_dataset_unzip_root}\"\n",
        "        return None\n",
        "\n",
        "    for split_folder_name in [\"train_folder\", \"testing_folder\"]:\n",
        "        split_folder_path = os.path.join(main_folder_path, split_folder_name)\n",
        "        if not os.path.isdir(split_folder_path): continue\n",
        "\n",
        "        for original_class_name in [\"Benign\", \"Normal\", \"InSitu\", \"Invasive\"]:\n",
        "            class_dir_path = os.path.join(split_folder_path, original_class_name)\n",
        "            if not os.path.isdir(class_dir_path): continue\n",
        "\n",
        "            for filename in os.listdir(class_dir_path):\n",
        "                if filename.lower().endswith((\".tif\", \".tiff\")):\n",
        "                    img_path = os.path.join(class_dir_path, filename)\n",
        "                    label_str = None\n",
        "                    original_class_lower = original_class_name.lower()\n",
        "                    if original_class_lower in ['benign', 'normal']:\n",
        "                        label_str = le_breakhis_classes_list[0]\n",
        "                    elif original_class_lower in ['insitu', 'invasive']:\n",
        "                        label_str = le_breakhis_classes_list[1]\n",
        "                    if label_str:\n",
        "                        all_image_data.append({'image_path': img_path, 'label_str': label_str})\n",
        "\n",
        "    if not all_image_data:\n",
        "        print(f\"ERROR: No images found in {main_folder_path} with expected subfolder structure.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Found {len(all_image_data)} total candidate images from BACH (dina0808) dataset before QC.\")\n",
        "\n",
        "    # --- Quality Control ---\n",
        "    qc_passed_data = []\n",
        "    for item_data in tqdm(all_image_data, desc=\"Quality Controlling BACH Patches\"):\n",
        "        img_path = item_data['image_path']\n",
        "        label_str = item_data['label_str']\n",
        "        try:\n",
        "            img_pil = Image.open(img_path).convert('RGB')\n",
        "            img_np = np.array(img_pil)\n",
        "\n",
        "            # 1. Blur Detection (Laplacian Variance)\n",
        "            gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n",
        "            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "            if laplacian_var < blur_threshold:\n",
        "                # print(f\"QC Fail (Blur): {os.path.basename(img_path)}, LVar: {laplacian_var:.2f}\")\n",
        "                continue # Skip this image\n",
        "\n",
        "            # 2. Tissue Detection (Simple Example: % of non-white/light-gray pixels)\n",
        "            # This threshold might need tuning based on how 'background' looks in BACH images\n",
        "            # A common background pixel might be around (240, 240, 240) or higher\n",
        "            # We count pixels that are NOT background.\n",
        "            bg_threshold_value = 220 # Pixels with all channels > this are considered background\n",
        "\n",
        "            # Check if image has 3 channels\n",
        "            if img_np.ndim < 3 or img_np.shape[2] < 3:\n",
        "                # print(f\"QC Fail (Channels): {os.path.basename(img_path)} does not have 3 channels. Shape: {img_np.shape}\")\n",
        "                continue\n",
        "\n",
        "\n",
        "            # Create a mask for non-background pixels\n",
        "            # A pixel is background if R > thresh AND G > thresh AND B > thresh\n",
        "            background_mask_pixels = (img_np[:,:,0] > bg_threshold_value) & \\\n",
        "                                     (img_np[:,:,1] > bg_threshold_value) & \\\n",
        "                                     (img_np[:,:,2] > bg_threshold_value)\n",
        "\n",
        "            tissue_mask_pixels = ~background_mask_pixels # Invert to get tissue pixels\n",
        "            tissue_percentage = np.mean(tissue_mask_pixels) * 100\n",
        "\n",
        "            if tissue_percentage < tissue_percentage_threshold:\n",
        "                # print(f\"QC Fail (Tissue%): {os.path.basename(img_path)}, Tissue: {tissue_percentage:.2f}%\")\n",
        "                continue # Skip this image\n",
        "\n",
        "            qc_passed_data.append({'image_path': img_path, 'label_str': label_str})\n",
        "        except Exception as e_qc:\n",
        "            print(f\"Error during QC for {img_path}: {e_qc}. Skipping.\")\n",
        "\n",
        "    if not qc_passed_data:\n",
        "        print(\"ERROR: No images passed quality control.\")\n",
        "        return None\n",
        "\n",
        "    qc_passed_df = pd.DataFrame(qc_passed_data)\n",
        "    print(f\"Found {len(qc_passed_df)} images after quality control.\")\n",
        "\n",
        "    # Encode string labels\n",
        "    temp_le = LabelEncoder()\n",
        "    temp_le.classes_ = np.array(le_breakhis_classes_list)\n",
        "    try:\n",
        "        qc_passed_df['encoded_label'] = temp_le.transform(qc_passed_df['label_str'])\n",
        "    except ValueError as e:\n",
        "        print(f\"ERROR encoding QC-passed dataset labels: {e}.\")\n",
        "        return None\n",
        "\n",
        "    # Apply overall subsetting AFTER QC\n",
        "    if subset_size is not None and subset_size < len(qc_passed_df):\n",
        "        print(f\"Sampling a final subset of {subset_size} images for evaluation...\")\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        final_df_subset, _ = train_test_split(\n",
        "            qc_passed_df,\n",
        "            train_size=subset_size,\n",
        "            stratify=qc_passed_df['encoded_label'],\n",
        "            random_state=SEED\n",
        "        )\n",
        "        print(f\"Prepared final metadata with {len(final_df_subset)} images (subset after QC).\")\n",
        "        return final_df_subset[['image_path', 'encoded_label']]\n",
        "    else:\n",
        "        if subset_size is not None:\n",
        "            print(f\"subset_size ({subset_size}) is >= QC-passed images ({len(qc_passed_df)}). Using all QC-passed images.\")\n",
        "        print(f\"Prepared final metadata with {len(qc_passed_df)} images (all QC-passed).\")\n",
        "        return qc_passed_df[['image_path', 'encoded_label']]\n",
        "\n",
        "print(\"BACH (dina0808) metadata preparation function with Quality Control defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrYZGqJ4X8GU",
        "outputId": "0d136f93-f214-4a35-db6d-0f6301155d8b"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BACH (dina0808) metadata preparation function with Quality Control defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_new_dataset_evaluation(): # Renamed back for generality\n",
        "    global breakhis_global_lab_stats\n",
        "    global KAGGLE_READY, MODEL_CHECKPOINT_PATH # Ensure these are accessible\n",
        "\n",
        "    if not os.path.exists(MODEL_CHECKPOINT_PATH):\n",
        "        print(f\"Model checkpoint '{MODEL_CHECKPOINT_PATH}' not available.\")\n",
        "        return\n",
        "\n",
        "    model_to_evaluate, le_classes_cp = load_trained_model_from_path_v0( # Your existing loader for no-mag model\n",
        "        MODEL_CHECKPOINT_PATH,\n",
        "        num_classes_expected=NUM_CLASSES_MODEL\n",
        "    )\n",
        "\n",
        "    if model_to_evaluate is None: return\n",
        "    current_le_classes = le_classes_cp if le_classes_cp is not None else ORIGINAL_BREAKHIS_LABEL_CLASSES\n",
        "    print(f\"Using LabelEncoder classes: {current_le_classes}\")\n",
        "\n",
        "    if breakhis_global_lab_stats is None:\n",
        "        print(\"BreaKHis LAB stats not available. Stain norm will use fallback or be skipped.\")\n",
        "        if 'breakhis_global_lab_stats' not in globals() or globals()['breakhis_global_lab_stats'] is None:\n",
        "             print(\"CRITICAL: Fallback LAB stats also not available. Exiting.\")\n",
        "             return\n",
        "\n",
        "    # --- Download New Dataset (dina0808/bach-icar-2018) ---\n",
        "    if not KAGGLE_READY:\n",
        "        print(\"Kaggle API not ready. Skipping new dataset download.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nDownloading BACH (dina0808/bach-icar-2018) dataset...\")\n",
        "    NEW_DATASET_UNZIP_DIR = \"/content/bach_dina0808_unzipped\" # New distinct name\n",
        "    NEW_DATASET_SLUG = \"dina0808/bach-icar-2018\" # <<<<----- NEW KAGGLE SLUG\n",
        "    NEW_DATASET_ZIP_FILENAME = NEW_DATASET_SLUG.split('/')[-1] + \".zip\" # e.g., \"bach-icar-2018.zip\"\n",
        "    NEW_DATASET_ZIP_PATH = f\"/content/{NEW_DATASET_ZIP_FILENAME}\"\n",
        "\n",
        "    # Check if already successfully unzipped (e.g., by checking for Photos/GroundTruth.csv)\n",
        "    expected_gt_file_in_unzip = os.path.join(NEW_DATASET_UNZIP_DIR, \"Photos\", \"GroundTruth.csv\")\n",
        "    # Some Kaggle datasets might have an extra top-level folder after unzipping\n",
        "    # Example: /content/bach_dina0808_unzipped/ICIAR2018_BACH_Challenge/Photos/GroundTruth.csv\n",
        "    expected_gt_file_in_unzip_nested = os.path.join(NEW_DATASET_UNZIP_DIR, \"ICIAR2018_BACH_Challenge\", \"Photos\", \"GroundTruth.csv\")\n",
        "\n",
        "\n",
        "    if os.path.exists(expected_gt_file_in_unzip) or os.path.exists(expected_gt_file_in_unzip_nested):\n",
        "        print(f\"New BACH dataset seems already unzipped in/under {NEW_DATASET_UNZIP_DIR}.\")\n",
        "    else:\n",
        "        if os.path.exists(NEW_DATASET_ZIP_PATH): os.remove(NEW_DATASET_ZIP_PATH)\n",
        "        if os.path.exists(NEW_DATASET_UNZIP_DIR): shutil.rmtree(NEW_DATASET_UNZIP_DIR)\n",
        "\n",
        "        print(f\"Downloading {NEW_DATASET_SLUG} to /content/ ...\")\n",
        "        !kaggle datasets download -d {NEW_DATASET_SLUG} -p /content/ -q --force\n",
        "\n",
        "        if not os.path.exists(NEW_DATASET_ZIP_PATH):\n",
        "            print(f\"ERROR: New dataset zip '{NEW_DATASET_ZIP_PATH}' not found after download.\")\n",
        "            !ls -lh /content/ # See what was actually downloaded\n",
        "            # Try common alternative name if slug-based name fails\n",
        "            alt_zip_name = \"archive.zip\"\n",
        "            if os.path.exists(f\"/content/{alt_zip_name}\"):\n",
        "                print(f\"Found '/content/{alt_zip_name}', assuming this is it and renaming.\")\n",
        "                os.rename(f\"/content/{alt_zip_name}\", NEW_DATASET_ZIP_PATH)\n",
        "            else:\n",
        "                print(\"No suitable zip file found for new dataset.\")\n",
        "                return\n",
        "\n",
        "        file_size_mb = os.path.getsize(NEW_DATASET_ZIP_PATH) / (1024*1024)\n",
        "        print(f\"New dataset zip file downloaded: {NEW_DATASET_ZIP_PATH}. Size: {file_size_mb:.2f} MB\")\n",
        "        # This dataset zip is ~2.34 GB. Min size check:\n",
        "        if file_size_mb < 2000: # approx 2GB\n",
        "             print(f\"WARNING: Downloaded new dataset zip size ({file_size_mb:.2f}MB) seems too small.\")\n",
        "\n",
        "        os.makedirs(NEW_DATASET_UNZIP_DIR, exist_ok=True)\n",
        "        print(f\"Unzipping to {NEW_DATASET_UNZIP_DIR}...\")\n",
        "        !unzip -q \"{NEW_DATASET_ZIP_PATH}\" -d \"{NEW_DATASET_UNZIP_DIR}\"\n",
        "        print(\"New dataset unzipped.\")\n",
        "\n",
        "        if not os.path.exists(NEW_DATASET_UNZIP_DIR) or not os.listdir(NEW_DATASET_UNZIP_DIR):\n",
        "            print(f\"ERROR: {NEW_DATASET_UNZIP_DIR} empty after unzip.\")\n",
        "            return\n",
        "\n",
        "    # Determine the actual content directory for metadata preparation\n",
        "    # This dataset (dina0808/bach-icar-2018) usually unzips directly into a structure\n",
        "    # where 'Photos' and 'GroundTruth.csv' (within Photos) are accessible relative to NEW_DATASET_UNZIP_DIR\n",
        "    # or sometimes within a single top-level folder like 'ICIAR2018_BACH_Challenge'.\n",
        "    actual_new_dataset_content_dir = NEW_DATASET_UNZIP_DIR\n",
        "    if os.path.isdir(os.path.join(NEW_DATASET_UNZIP_DIR, \"ICIAR2018_BACH_Challenge\")) and \\\n",
        "       os.path.exists(os.path.join(NEW_DATASET_UNZIP_DIR, \"ICIAR2018_BACH_Challenge\", \"Photos\", \"GroundTruth.csv\")):\n",
        "        actual_new_dataset_content_dir = os.path.join(NEW_DATASET_UNZIP_DIR, \"ICIAR2018_BACH_Challenge\")\n",
        "    elif not (os.path.isdir(os.path.join(NEW_DATASET_UNZIP_DIR, \"Photos\")) and \\\n",
        "              os.path.exists(os.path.join(NEW_DATASET_UNZIP_DIR, \"Photos\", \"GroundTruth.csv\"))):\n",
        "        print(f\"Warning: Standard BACH structure (Photos/GroundTruth.csv or ICIAR2018_BACH_Challenge/Photos/GroundTruth.csv) not immediately found in {NEW_DATASET_UNZIP_DIR}.\")\n",
        "        print(\"Listing contents to help debug:\")\n",
        "        !ls -R \"{NEW_DATASET_UNZIP_DIR}\"\n",
        "        # Fallback: assume metadata prep can handle NEW_DATASET_UNZIP_DIR as its base.\n",
        "        # This might need adjustment in prepare_bach_metadata_v0 if structure is very different.\n",
        "\n",
        "    print(f\"Using {actual_new_dataset_content_dir} as base for new dataset metadata preparation.\")\n",
        "\n",
        "    BACH_SUBSET_SIZE_AFTER_QC = 500 # Define how many images you want to test on *after* QC\n",
        "    BLUR_THRESHOLD_BACH = 75.0  # Tune this by looking at Laplacian values of some good/bad images\n",
        "    TISSUE_PERCENTAGE_THRESHOLD_BACH = 40.0 # Min % of tissue pixels\n",
        "\n",
        "    print(f\"Will prepare metadata for a subset of up to {BACH_SUBSET_SIZE_AFTER_QC} BACH images after QC.\")\n",
        "    new_dataset_metadata_df = prepare_bach_metadata_v0( # Call the updated function\n",
        "        actual_new_dataset_content_dir, # Base directory of unzipped BACH data\n",
        "        current_le_classes,\n",
        "        subset_size=BACH_SUBSET_SIZE_AFTER_QC,\n",
        "        blur_threshold=BLUR_THRESHOLD_BACH,\n",
        "        tissue_percentage_threshold=TISSUE_PERCENTAGE_THRESHOLD_BACH\n",
        "    )\n",
        "\n",
        "\n",
        "    if new_dataset_metadata_df is None or new_dataset_metadata_df.empty:\n",
        "        print(\"Failed to prepare metadata for the BACH dataset (after QC/subsetting). Skipping evaluation.\")\n",
        "        return\n",
        "    # Create Dataset for TTA using the NEW dataset's metadata\n",
        "    NEW_DATASET_SUBSET_SIZE = 500 # Or however many you want for testing\n",
        "    if len(new_dataset_metadata_df) > NEW_DATASET_SUBSET_SIZE:\n",
        "\n",
        "        print(f\"Taking a subset of {NEW_DATASET_SUBSET_SIZE} from the new dataset for TTA.\")\n",
        "        # Stratified sampling\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        new_dataset_metadata_subset_df, _ = train_test_split(\n",
        "            new_dataset_metadata_df,\n",
        "            train_size=NEW_DATASET_SUBSET_SIZE,\n",
        "            stratify=new_dataset_metadata_df['encoded_label'],\n",
        "            random_state=SEED\n",
        "        )\n",
        "    else:\n",
        "        new_dataset_metadata_subset_df = new_dataset_metadata_df\n",
        "        print(f\"Using all {len(new_dataset_metadata_subset_df)} available images from the new dataset for TTA.\")\n",
        "\n",
        "\n",
        "    new_eval_dataset_tta = NewDatasetForTTA(\n",
        "        metadata_df=new_dataset_metadata_df,\n",
        "        use_stain_normalization=True, # Assuming you still want this\n",
        "        target_lab_stats_for_norm=breakhis_global_lab_stats # Using BreaKHis stats\n",
        "    )\n",
        "    print(f\"Created BACH dataset for TTA with {len(new_eval_dataset_tta)} images.\")\n",
        "    print(f\"Created new dataset for TTA with {len(new_eval_dataset_tta)} images.\")\n",
        "\n",
        "    # Evaluate on New Dataset with TTA\n",
        "    print(\"\\nStarting evaluation on the new (BACH - dina0808) dataset with TTA...\")\n",
        "    # ... (The TTA evaluation loop and metrics printing code from your previous Cell 7\n",
        "    #      can be used here. Just ensure variable names like all_preds_new_tta,\n",
        "    #      all_labels_new_tta, etc. are used, and titles for plots/prints reflect\n",
        "    #      that it's this new dataset.) ...\n",
        "    all_preds_new_tta, all_labels_new_tta, all_tta_avg_probs_new = [], [], []\n",
        "    model_to_evaluate.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(len(new_eval_dataset_tta)), desc=\"TTA on New Dataset\"):\n",
        "            data_tuple = new_eval_dataset_tta[i]\n",
        "            original_pil_image, label = data_tuple[0], data_tuple[1]\n",
        "            if original_pil_image is None: continue\n",
        "\n",
        "            tta_image_tensors = []\n",
        "            for tta_transform_pil in tta_pil_transforms:\n",
        "                augmented_pil = tta_transform_pil(original_pil_image)\n",
        "                image_tensor = final_tta_processing_transform(augmented_pil)\n",
        "                tta_image_tensors.append(image_tensor)\n",
        "            if not tta_image_tensors: continue\n",
        "\n",
        "            tta_batch = torch.stack(tta_image_tensors).to(DEVICE)\n",
        "            outputs = model_to_evaluate(tta_batch)\n",
        "            probabilities = torch.softmax(outputs, dim=1)\n",
        "            avg_probabilities = torch.mean(probabilities, dim=0)\n",
        "            _, predicted_class_idx = torch.max(avg_probabilities, dim=0)\n",
        "\n",
        "            all_preds_new_tta.append(predicted_class_idx.cpu().item())\n",
        "            all_labels_new_tta.append(label)\n",
        "            all_tta_avg_probs_new.append(avg_probabilities.cpu().numpy()[1])\n",
        "\n",
        "    if all_labels_new_tta:\n",
        "        print(\"\\n--- New Dataset (BACH - dina0808) Evaluation Results (TTA, BreaKHis Stats Norm) ---\")\n",
        "        new_accuracy = accuracy_score(all_labels_new_tta, all_preds_new_tta)\n",
        "        new_auc = 0.0\n",
        "        if len(np.unique(all_labels_new_tta)) > 1:\n",
        "             new_auc = roc_auc_score(all_labels_new_tta, all_tta_avg_probs_new)\n",
        "        else: print(\"Warning: Could not compute AUC for new dataset (single class).\")\n",
        "        print(f\"  Accuracy:  {new_accuracy * 100:.2f}%\")\n",
        "        print(f\"  AUC:       {new_auc:.4f}\")\n",
        "        new_precision = precision_score(all_labels_new_tta, all_preds_new_tta, zero_division=0)\n",
        "        new_recall = recall_score(all_labels_new_tta, all_preds_new_tta, zero_division=0)\n",
        "        new_f1 = f1_score(all_labels_new_tta, all_preds_new_tta, zero_division=0)\n",
        "        print(f\"  Precision: {new_precision:.4f}\")\n",
        "        print(f\"  Recall:    {new_recall:.4f}\")\n",
        "        print(f\"  F1 Score:  {new_f1:.4f}\")\n",
        "\n",
        "        print(\"\\nClassification Report (New Dataset - BACH dina0808 with TTA):\")\n",
        "        print(classification_report(all_labels_new_tta, all_preds_new_tta, target_names=current_le_classes, zero_division=0))\n",
        "        cm_new = confusion_matrix(all_labels_new_tta, all_preds_new_tta)\n",
        "        plt.figure(figsize=(6,5)); sns.heatmap(cm_new, annot=True, fmt='d', cmap='Blues', xticklabels=current_le_classes, yticklabels=current_le_classes)\n",
        "        plt.xlabel('Predicted'); plt.ylabel('True'); plt.title('Confusion Matrix - New Dataset (BACH dina0808 TTA)'); plt.show()\n",
        "    else:\n",
        "        print(\"No valid predictions for new dataset TTA evaluation.\")"
      ],
      "metadata": {
        "id": "BmwN3RoaX_K3"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 8: Main Execution - Trigger Evaluation on New (BACH - dina0808) Dataset ---\n",
        "\n",
        "if __name__ == '__main__' and '__file__' not in locals(): # Standard Colab check to run code\n",
        "    print(\"===================================================\")\n",
        "    print(\"STARTING EVALUATION ON NEW (BACH - dina0808) DATASET\")\n",
        "    print(\"===================================================\")\n",
        "\n",
        "    # --- Sanity Checks for Critical Global Variables ---\n",
        "    # These should have been set by previous cells.\n",
        "    # MODEL_CHECKPOINT_PATH should be defined in Cell 2 (e.g., \"best_phased_model_checkpoint.pth\")\n",
        "    # and the file should have been uploaded to Colab session storage.\n",
        "\n",
        "    all_prerequisites_met = True\n",
        "\n",
        "    # 1. Model Checkpoint Path\n",
        "    if 'MODEL_CHECKPOINT_PATH' not in globals():\n",
        "        print(\"ERROR: Global variable MODEL_CHECKPOINT_PATH is not defined. Please define it in Cell 2.\")\n",
        "        all_prerequisites_met = False\n",
        "    elif not os.path.exists(MODEL_CHECKPOINT_PATH):\n",
        "        print(f\"ERROR: Model checkpoint file '{MODEL_CHECKPOINT_PATH}' does not exist in Colab session storage.\")\n",
        "        print(\"Please upload your '.pth' model file and ensure MODEL_CHECKPOINT_PATH in Cell 2 is correct.\")\n",
        "        all_prerequisites_met = False\n",
        "\n",
        "    # 2. Kaggle API Readiness\n",
        "    if 'KAGGLE_READY' not in globals() or not KAGGLE_READY:\n",
        "        print(\"ERROR: KAGGLE_READY is not True. Please ensure Kaggle API (kaggle.json) was set up successfully in Cell 3.\")\n",
        "        all_prerequisites_met = False\n",
        "\n",
        "    # 3. BreaKHis Normalization Stats\n",
        "    if 'breakhis_global_lab_stats' not in globals() or breakhis_global_lab_stats is None:\n",
        "        print(\"ERROR: breakhis_global_lab_stats is not available.\")\n",
        "        print(\"Please ensure Cell 3 (BreaKHis data processing and stats calculation) ran successfully and calculated these stats.\")\n",
        "        all_prerequisites_met = False\n",
        "\n",
        "    # 4. Check for essential functions and class definitions from previous cells\n",
        "    required_definitions = [\n",
        "        'load_trained_model_from_path_v0',        # From Cell 6\n",
        "        'prepare_bach_metadata_v0',               # From Cell 6 (for BACH structure)\n",
        "        'NewDatasetForTTA',                       # From Cell 5\n",
        "        'opencv_reinhard_normalize',              # From Cell 5\n",
        "        'run_new_dataset_evaluation',             # From Cell 7 (updated for dina0808/bach-icar-2018)\n",
        "        'HybridModel',                            # From Cell 4 (version without mag embedding)\n",
        "        'tta_pil_transforms',                     # From Cell 7 (or a dedicated TTA cell)\n",
        "        'final_tta_processing_transform'          # From Cell 7 (or a dedicated TTA cell)\n",
        "    ]\n",
        "    for func_name in required_definitions:\n",
        "        if func_name not in globals():\n",
        "            print(f\"ERROR: Required function or class '{func_name}' is not defined. Please run all preceding cells.\")\n",
        "            all_prerequisites_met = False\n",
        "            break\n",
        "\n",
        "    # 5. Check for essential config variables from Cell 2\n",
        "    required_configs = [\n",
        "        'DEVICE', 'NUM_CLASSES_MODEL', 'ORIGINAL_BREAKHIS_LABEL_CLASSES',\n",
        "        'IMAGENET_MEAN', 'IMAGENET_STD'\n",
        "    ]\n",
        "    for config_name in required_configs:\n",
        "        if config_name not in globals():\n",
        "            print(f\"ERROR: Required configuration variable '{config_name}' is not defined. Please ensure Cell 2 ran.\")\n",
        "            all_prerequisites_met = False\n",
        "            break\n",
        "\n",
        "    if all_prerequisites_met:\n",
        "        print(\"\\nAll prerequisite checks passed. Proceeding with evaluation on the BACH (dina0808) dataset...\")\n",
        "        try:\n",
        "            # The run_new_dataset_evaluation function is now tailored for the\n",
        "            # \"dina0808/bach-icar-2018\" Kaggle dataset.\n",
        "            run_new_dataset_evaluation() # This function is defined in your Cell 7\n",
        "            print(\"\\n===================================================\")\n",
        "            print(\"EVALUATION ON NEW (BACH - dina0808) DATASET COMPLETE.\")\n",
        "            print(\"===================================================\")\n",
        "        except Exception as e_main_eval:\n",
        "            print(f\"An unexpected error occurred during run_new_dataset_evaluation: {e_main_eval}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "    else:\n",
        "        print(\"\\nOne or more prerequisites failed. Please review the error messages above and ensure all previous cells ran correctly.\")\n",
        "\n",
        "elif '__file__' in locals():\n",
        "    print(\"Script is being imported. Skipping main execution block for BACH (dina0808) evaluation.\")\n",
        "else:\n",
        "    print(\"Not in a main execution context (e.g., Colab cell run directly). To run, execute this cell after all setups.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "cfa3f557d69d4602ab545bf61560bc6c",
            "ff820a5ba8ab40a0a48036b9b3916c28",
            "5e7bdc4bca2b4d239c35602609d332f4",
            "32ca5b64b59f4c559523264a78326127",
            "f33143b4fb7e4530956ba666b1e00337",
            "75e0102a32ba493cb855a59ae1782a46",
            "af11b17cc31248bb84918f2358f015d2",
            "588cdd94aecd417b9152dcf6156e0458",
            "4cab4c6baafd461ca27d70f3d19e1da6",
            "2a4581cbbfcf47099959d8446acda64c",
            "58c18a22d0c44b03a041ac7ecfa93d7e",
            "d00679195f9d4d69833bf81c182758c4",
            "b9595e32df28441aafb63a468f09e05d",
            "6a17136c754e4b8b877eba25c2305d78",
            "0534529dbab14f0fa960266211d61b50",
            "283c8ae248904365b1838f811ada36b7",
            "1c339c3ed3864e2fa087913c22ec5edf",
            "2a1fd8ab0eb448539a08ecb7ffac4dd6",
            "4ff7291d61414e96a7dd544de21af40d",
            "a468c86b57ca4f2bb86907972f3062a6",
            "a7c83998645244d9b043276375e6929b",
            "e099e0fa0424459cbf750394c098bccc"
          ]
        },
        "id": "DwDhg1KHQZRp",
        "outputId": "76e8f5ef-5685-4e81-8872-9b769f63145e"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===================================================\n",
            "STARTING EVALUATION ON NEW (BACH - dina0808) DATASET\n",
            "===================================================\n",
            "\n",
            "All prerequisite checks passed. Proceeding with evaluation on the BACH (dina0808) dataset...\n",
            "Loading model checkpoint from: best_phased_model_checkpoint.pth\n",
            "Model state_dict loaded successfully (to CPU model instance).\n",
            "Model moved to cuda.\n",
            "Using LabelEncoder classes: ['benign', 'malignant']\n",
            "\n",
            "Downloading BACH (dina0808/bach-icar-2018) dataset...\n",
            "Downloading dina0808/bach-icar-2018 to /content/ ...\n",
            "Dataset URL: https://www.kaggle.com/datasets/dina0808/bach-icar-2018\n",
            "License(s): unknown\n",
            "Downloading bach-icar-2018.zip to /content\n",
            "100% 3.94G/3.95G [01:06<00:00, 141MB/s]\n",
            "100% 3.95G/3.95G [01:06<00:00, 63.9MB/s]\n",
            "New dataset zip file downloaded: /content/bach-icar-2018.zip. Size: 4049.65 MB\n",
            "Unzipping to /content/bach_dina0808_unzipped...\n",
            "New dataset unzipped.\n",
            "Warning: Standard BACH structure (Photos/GroundTruth.csv or ICIAR2018_BACH_Challenge/Photos/GroundTruth.csv) not immediately found in /content/bach_dina0808_unzipped.\n",
            "Listing contents to help debug:\n",
            "/content/bach_dina0808_unzipped:\n",
            "main_folder\n",
            "\n",
            "/content/bach_dina0808_unzipped/main_folder:\n",
            "testing_folder\ttrain_folder\n",
            "\n",
            "/content/bach_dina0808_unzipped/main_folder/testing_folder:\n",
            "Benign\tInSitu\tInvasive  Normal\n",
            "\n",
            "/content/bach_dina0808_unzipped/main_folder/testing_folder/Benign:\n",
            "b001.tif  b003.tif  b005.tif  b007.tif\tb009.tif\n",
            "b002.tif  b004.tif  b006.tif  b008.tif\tb010.tif\n",
            "\n",
            "/content/bach_dina0808_unzipped/main_folder/testing_folder/InSitu:\n",
            "is001.tif  is003.tif  is005.tif  is007.tif  is009.tif\n",
            "is002.tif  is004.tif  is006.tif  is008.tif  is010.tif\n",
            "\n",
            "/content/bach_dina0808_unzipped/main_folder/testing_folder/Invasive:\n",
            "iv001.tif  iv003.tif  iv005.tif  iv007.tif  iv009.tif\n",
            "iv002.tif  iv004.tif  iv006.tif  iv008.tif  iv010.tif\n",
            "\n",
            "/content/bach_dina0808_unzipped/main_folder/testing_folder/Normal:\n",
            "n001.tif  n003.tif  n005.tif  n007.tif\tn009.tif\n",
            "n002.tif  n004.tif  n006.tif  n008.tif\tn010.tif\n",
            "\n",
            "/content/bach_dina0808_unzipped/main_folder/train_folder:\n",
            "Benign\tInSitu\tInvasive  Normal\n",
            "\n",
            "/content/bach_dina0808_unzipped/main_folder/train_folder/Benign:\n",
            "b001.tif  b014.tif  b027.tif  b040.tif\tb053.tif  b066.tif  b079.tif  b092.tif\n",
            "b002.tif  b015.tif  b028.tif  b041.tif\tb054.tif  b067.tif  b080.tif  b093.tif\n",
            "b003.tif  b016.tif  b029.tif  b042.tif\tb055.tif  b068.tif  b081.tif  b094.tif\n",
            "b004.tif  b017.tif  b030.tif  b043.tif\tb056.tif  b069.tif  b082.tif  b095.tif\n",
            "b005.tif  b018.tif  b031.tif  b044.tif\tb057.tif  b070.tif  b083.tif  b096.tif\n",
            "b006.tif  b019.tif  b032.tif  b045.tif\tb058.tif  b071.tif  b084.tif  b097.tif\n",
            "b007.tif  b020.tif  b033.tif  b046.tif\tb059.tif  b072.tif  b085.tif  b098.tif\n",
            "b008.tif  b021.tif  b034.tif  b047.tif\tb060.tif  b073.tif  b086.tif  b099.tif\n",
            "b009.tif  b022.tif  b035.tif  b048.tif\tb061.tif  b074.tif  b087.tif  b100.tif\n",
            "b010.tif  b023.tif  b036.tif  b049.tif\tb062.tif  b075.tif  b088.tif  Thumbs.db\n",
            "b011.tif  b024.tif  b037.tif  b050.tif\tb063.tif  b076.tif  b089.tif\n",
            "b012.tif  b025.tif  b038.tif  b051.tif\tb064.tif  b077.tif  b090.tif\n",
            "b013.tif  b026.tif  b039.tif  b052.tif\tb065.tif  b078.tif  b091.tif\n",
            "\n",
            "/content/bach_dina0808_unzipped/main_folder/train_folder/InSitu:\n",
            "is001.tif  is016.tif  is031.tif  is046.tif  is061.tif  is076.tif  is091.tif\n",
            "is002.tif  is017.tif  is032.tif  is047.tif  is062.tif  is077.tif  is092.tif\n",
            "is003.tif  is018.tif  is033.tif  is048.tif  is063.tif  is078.tif  is093.tif\n",
            "is004.tif  is019.tif  is034.tif  is049.tif  is064.tif  is079.tif  is094.tif\n",
            "is005.tif  is020.tif  is035.tif  is050.tif  is065.tif  is080.tif  is095.tif\n",
            "is006.tif  is021.tif  is036.tif  is051.tif  is066.tif  is081.tif  is096.tif\n",
            "is007.tif  is022.tif  is037.tif  is052.tif  is067.tif  is082.tif  is097.tif\n",
            "is008.tif  is023.tif  is038.tif  is053.tif  is068.tif  is083.tif  is098.tif\n",
            "is009.tif  is024.tif  is039.tif  is054.tif  is069.tif  is084.tif  is099.tif\n",
            "is010.tif  is025.tif  is040.tif  is055.tif  is070.tif  is085.tif  is100.tif\n",
            "is011.tif  is026.tif  is041.tif  is056.tif  is071.tif  is086.tif  Thumbs.db\n",
            "is012.tif  is027.tif  is042.tif  is057.tif  is072.tif  is087.tif\n",
            "is013.tif  is028.tif  is043.tif  is058.tif  is073.tif  is088.tif\n",
            "is014.tif  is029.tif  is044.tif  is059.tif  is074.tif  is089.tif\n",
            "is015.tif  is030.tif  is045.tif  is060.tif  is075.tif  is090.tif\n",
            "\n",
            "/content/bach_dina0808_unzipped/main_folder/train_folder/Invasive:\n",
            "iv001.tif  iv016.tif  iv031.tif  iv046.tif  iv061.tif  iv076.tif  iv091.tif\n",
            "iv002.tif  iv017.tif  iv032.tif  iv047.tif  iv062.tif  iv077.tif  iv092.tif\n",
            "iv003.tif  iv018.tif  iv033.tif  iv048.tif  iv063.tif  iv078.tif  iv093.tif\n",
            "iv004.tif  iv019.tif  iv034.tif  iv049.tif  iv064.tif  iv079.tif  iv094.tif\n",
            "iv005.tif  iv020.tif  iv035.tif  iv050.tif  iv065.tif  iv080.tif  iv095.tif\n",
            "iv006.tif  iv021.tif  iv036.tif  iv051.tif  iv066.tif  iv081.tif  iv096.tif\n",
            "iv007.tif  iv022.tif  iv037.tif  iv052.tif  iv067.tif  iv082.tif  iv097.tif\n",
            "iv008.tif  iv023.tif  iv038.tif  iv053.tif  iv068.tif  iv083.tif  iv098.tif\n",
            "iv009.tif  iv024.tif  iv039.tif  iv054.tif  iv069.tif  iv084.tif  iv099.tif\n",
            "iv010.tif  iv025.tif  iv040.tif  iv055.tif  iv070.tif  iv085.tif  iv100.tif\n",
            "iv011.tif  iv026.tif  iv041.tif  iv056.tif  iv071.tif  iv086.tif  Thumbs.db\n",
            "iv012.tif  iv027.tif  iv042.tif  iv057.tif  iv072.tif  iv087.tif\n",
            "iv013.tif  iv028.tif  iv043.tif  iv058.tif  iv073.tif  iv088.tif\n",
            "iv014.tif  iv029.tif  iv044.tif  iv059.tif  iv074.tif  iv089.tif\n",
            "iv015.tif  iv030.tif  iv045.tif  iv060.tif  iv075.tif  iv090.tif\n",
            "\n",
            "/content/bach_dina0808_unzipped/main_folder/train_folder/Normal:\n",
            "n001.tif  n014.tif  n027.tif  n040.tif\tn053.tif  n066.tif  n079.tif  n092.tif\n",
            "n002.tif  n015.tif  n028.tif  n041.tif\tn054.tif  n067.tif  n080.tif  n093.tif\n",
            "n003.tif  n016.tif  n029.tif  n042.tif\tn055.tif  n068.tif  n081.tif  n094.tif\n",
            "n004.tif  n017.tif  n030.tif  n043.tif\tn056.tif  n069.tif  n082.tif  n095.tif\n",
            "n005.tif  n018.tif  n031.tif  n044.tif\tn057.tif  n070.tif  n083.tif  n096.tif\n",
            "n006.tif  n019.tif  n032.tif  n045.tif\tn058.tif  n071.tif  n084.tif  n097.tif\n",
            "n007.tif  n020.tif  n033.tif  n046.tif\tn059.tif  n072.tif  n085.tif  n098.tif\n",
            "n008.tif  n021.tif  n034.tif  n047.tif\tn060.tif  n073.tif  n086.tif  n099.tif\n",
            "n009.tif  n022.tif  n035.tif  n048.tif\tn061.tif  n074.tif  n087.tif  n100.tif\n",
            "n010.tif  n023.tif  n036.tif  n049.tif\tn062.tif  n075.tif  n088.tif  Thumbs.db\n",
            "n011.tif  n024.tif  n037.tif  n050.tif\tn063.tif  n076.tif  n089.tif\n",
            "n012.tif  n025.tif  n038.tif  n051.tif\tn064.tif  n077.tif  n090.tif\n",
            "n013.tif  n026.tif  n039.tif  n052.tif\tn065.tif  n078.tif  n091.tif\n",
            "Using /content/bach_dina0808_unzipped as base for new dataset metadata preparation.\n",
            "Will prepare metadata for a subset of up to 500 BACH images after QC.\n",
            "Preparing BACH (dina0808) metadata from unzipped root: /content/bach_dina0808_unzipped\n",
            "QC Params: Blur Threshold=75.0, Tissue Area Threshold=40.0%\n",
            "Found 440 total candidate images from BACH (dina0808) dataset before QC.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Quality Controlling BACH Patches:   0%|          | 0/440 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cfa3f557d69d4602ab545bf61560bc6c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 275 images after quality control.\n",
            "subset_size (500) is >= QC-passed images (275). Using all QC-passed images.\n",
            "Prepared final metadata with 275 images (all QC-passed).\n",
            "Using all 275 available images from the new dataset for TTA.\n",
            "Created BACH dataset for TTA with 275 images.\n",
            "Created new dataset for TTA with 275 images.\n",
            "\n",
            "Starting evaluation on the new (BACH - dina0808) dataset with TTA...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "TTA on New Dataset:   0%|          | 0/275 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d00679195f9d4d69833bf81c182758c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- New Dataset (BACH - dina0808) Evaluation Results (TTA, BreaKHis Stats Norm) ---\n",
            "  Accuracy:  63.27%\n",
            "  AUC:       0.6280\n",
            "  Precision: 0.6781\n",
            "  Recall:    0.6471\n",
            "  F1 Score:  0.6622\n",
            "\n",
            "Classification Report (New Dataset - BACH dina0808 with TTA):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      benign       0.58      0.61      0.60       122\n",
            "   malignant       0.68      0.65      0.66       153\n",
            "\n",
            "    accuracy                           0.63       275\n",
            "   macro avg       0.63      0.63      0.63       275\n",
            "weighted avg       0.64      0.63      0.63       275\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================================\n",
            "EVALUATION ON NEW (BACH - dina0808) DATASET COMPLETE.\n",
            "===================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZSSXUKoJhUjB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}